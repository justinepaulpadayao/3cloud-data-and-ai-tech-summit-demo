# This is a Databricks asset bundle definition for tech-summit-demo.
# The Databricks extension requires databricks.yml configuration file.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.

bundle:
  name: tech-summit-demo

variables:
  catalog_name:
    description: Unity Catalog name
    default: tech_summit_demo
  
  schema_name:
    description: Schema name for the environment
    default: default

  cluster_name:
    description: Cluster name for jobs
    default: tech-summit-demo-cluster

include:
  - resources/*.yml

artifacts:
  default:
    type: whl
    build: poetry build
    path: .

resources:
  jobs:
    bronze_ingestion_job:
      name: "Bronze Layer Ingestion - ${bundle.target}"
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
      timeout_seconds: 3600
      max_concurrent_runs: 1
      tasks:
        - task_key: bronze_ingestion
          existing_cluster_id: ${var.cluster_id}
          python_wheel_task:
            package_name: tech_summit_demo
            entry_point: bronze_ingestion
          libraries:
            - whl: ../dist/*.whl

    silver_transformation_job:
      name: "Silver Layer Transformation - ${bundle.target}"
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
      timeout_seconds: 3600
      max_concurrent_runs: 1
      tasks:
        - task_key: accounts_silver
          existing_cluster_id: ${var.cluster_id}
          python_wheel_task:
            package_name: tech_summit_demo
            entry_point: accounts_silver
          libraries:
            - whl: ../dist/*.whl
        - task_key: customers_silver
          existing_cluster_id: ${var.cluster_id}
          python_wheel_task:
            package_name: tech_summit_demo
            entry_point: customers_silver
          libraries:
            - whl: ../dist/*.whl
          depends_on:
            - task_key: accounts_silver
        - task_key: transactions_silver
          existing_cluster_id: ${var.cluster_id}
          python_wheel_task:
            package_name: tech_summit_demo
            entry_point: transactions_silver
          libraries:
            - whl: ../dist/*.whl
          depends_on:
            - task_key: accounts_silver

    gold_aggregation_job:
      name: "Gold Layer Aggregation - ${bundle.target}"
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
      timeout_seconds: 3600
      max_concurrent_runs: 1
      tasks:
        - task_key: accounts_gold
          existing_cluster_id: ${var.cluster_id}
          python_wheel_task:
            package_name: tech_summit_demo
            entry_point: accounts_gold
          libraries:
            - whl: ../dist/*.whl
        - task_key: customers_gold
          existing_cluster_id: ${var.cluster_id}
          python_wheel_task:
            package_name: tech_summit_demo
            entry_point: customers_gold
          libraries:
            - whl: ../dist/*.whl
        - task_key: transactions_gold
          existing_cluster_id: ${var.cluster_id}
          python_wheel_task:
            package_name: tech_summit_demo
            entry_point: transactions_gold
          libraries:
            - whl: ../dist/*.whl

    data_quality_validation_job:
      name: "Data Quality Validation - ${bundle.target}"
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
      timeout_seconds: 1800
      max_concurrent_runs: 1
      tasks:
        - task_key: run_data_quality_tests
          existing_cluster_id: ${var.cluster_id}
          python_wheel_task:
            package_name: tech_summit_demo
            entry_point: data_quality_tests
          libraries:
            - whl: ../dist/*.whl

  experiments:
    tech_summit_demo_experiment:
      name: /Shared/tech-summit-demo/${bundle.target}/experiments
      description: MLflow experiments for ${bundle.target} environment

targets:
  dev:
    mode: development
    default: true
    workspace:
      host: https://adb-1197831686322840.0.azuredatabricks.net
      root_path: /Workspace/Users/${workspace.current_user.userName}/tech-summit-demo
    variables:
      catalog_name: tech_summit_demo_dev
      schema_name: development
      cluster_id: ${resources.clusters.dev_cluster.id}
    resources:
      clusters:
        dev_cluster:
          cluster_name: "tech-summit-demo-dev-cluster"
          spark_version: "13.3.x-scala2.12"
          node_type_id: "Standard_DS3_v2"
          num_workers: 1
          autotermination_minutes: 30
          spark_conf:
            "spark.databricks.delta.preview.enabled": "true"
            "spark.sql.adaptive.enabled": "true"
            "spark.sql.adaptive.coalescePartitions.enabled": "true"

  staging:
    mode: production
    workspace:
      host: https://adb-1197831686322840.0.azuredatabricks.net
      root_path: /Shared/tech-summit-demo/staging
    variables:
      catalog_name: tech_summit_demo_staging
      schema_name: staging
      cluster_id: ${resources.clusters.staging_cluster.id}
    resources:
      clusters:
        staging_cluster:
          cluster_name: "tech-summit-demo-staging-cluster"
          spark_version: "13.3.x-scala2.12"
          node_type_id: "Standard_DS3_v2"
          num_workers: 2
          autotermination_minutes: 60
          spark_conf:
            "spark.databricks.delta.preview.enabled": "true"
            "spark.sql.adaptive.enabled": "true"
            "spark.sql.adaptive.coalescePartitions.enabled": "true"

  prod:
    mode: production
    workspace:
      host: https://adb-1197831686322840.0.azuredatabricks.net
      root_path: /Shared/tech-summit-demo/production
    variables:
      catalog_name: tech_summit_demo_prod
      schema_name: production
      cluster_id: ${resources.clusters.prod_cluster.id}
    resources:
      clusters:
        prod_cluster:
          cluster_name: "tech-summit-demo-prod-cluster"
          spark_version: "13.3.x-scala2.12"
          node_type_id: "Standard_DS4_v2"
          num_workers: 4
          autotermination_minutes: 120
          spark_conf:
            "spark.databricks.delta.preview.enabled": "true"
            "spark.sql.adaptive.enabled": "true"
            "spark.sql.adaptive.coalescePartitions.enabled": "true"
            "spark.databricks.delta.autoCompact.enabled": "true"
